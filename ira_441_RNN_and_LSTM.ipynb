{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ira_441_RNN_and_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "U4-S3-DNN (Python 3.7)",
      "language": "python",
      "name": "u4-s3-dnn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragmatizt/DS-Unit-4-Sprint-3-Deep-Learning/blob/master/ira_441_RNN_and_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_IizNKWLomoA"
      },
      "source": [
        "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
        "\n",
        "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
        "## _aka_ PREDICTING THE FUTURE!\n",
        "\n",
        "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
        "<br></br>\n",
        "<br></br>\n",
        "\n",
        "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
        "\n",
        "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
        "\n",
        "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
        "\n",
        "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44QZgrPUe3-Y"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
        "\n",
        "$F_n = F_{n-1} + F_{n-2}$\n",
        "\n",
        "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
        "\n",
        "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
        "\n",
        "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
        "\n",
        "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
        "\n",
        "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
        "\n",
        "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
        "\n",
        "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
        "\n",
        "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
        "\n",
        "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
        "\n",
        "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
        "- https://keras.io/layers/recurrent/#lstm\n",
        "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
        "\n",
        "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eWrQllf8WEd-"
      },
      "source": [
        "### RNN/LSTM Sentiment Classification with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ti23G0gRe3kr",
        "outputId": "3d4475da-ff37-4e75-d755-2930bd7b994b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "'''\n",
        "#Trains an LSTM model on the IMDB sentiment classification task.\n",
        "The dataset is actually too small for LSTM to be of any advantage\n",
        "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
        "**Notes**\n",
        "- RNNs are tricky. Choice of batch size is important,\n",
        "choice of loss and optimizer is critical, etc.\n",
        "Some configurations won't converge.\n",
        "- LSTM loss decrease patterns during training can be quite different\n",
        "from what you see with CNNs/MLPs/etc.\n",
        "'''\n",
        "from __future__ import print_function\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "max_features = 20000\n",
        "# cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 80\n",
        "batch_size = 32\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eenZUmk4PvN",
        "colab_type": "code",
        "outputId": "34f0c4c3-8c4a-4be2-8e96-6fd95c355a2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(x_train)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1b955d7d-078c-4c65-8269-d9c4299a6979",
        "id": "DuiMMzQl4PvS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 80)\n",
            "x_test shape: (25000, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI3zbMAN4PvW",
        "colab_type": "code",
        "outputId": "fd8fe391-a58a-48a8-d0ba-1d9ddd27afd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
              "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
              "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
              "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
              "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
              "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
              "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
              "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
              "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "35d41070-5d80-40de-af66-42f20a967e07",
        "id": "oJiYK4794Pvh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        }
      },
      "source": [
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# try using different optimizers and different optimizer configs\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print('Train...')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=15,\n",
        "          validation_data=(x_test, y_test))\n",
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/15\n",
            "25000/25000 [==============================] - 112s 4ms/sample - loss: 0.4562 - acc: 0.7885 - val_loss: 0.3955 - val_acc: 0.8305\n",
            "Epoch 2/15\n",
            "25000/25000 [==============================] - 110s 4ms/sample - loss: 0.2998 - acc: 0.8783 - val_loss: 0.4060 - val_acc: 0.8247\n",
            "Epoch 3/15\n",
            "25000/25000 [==============================] - 111s 4ms/sample - loss: 0.2110 - acc: 0.9179 - val_loss: 0.4331 - val_acc: 0.8168\n",
            "Epoch 4/15\n",
            "25000/25000 [==============================] - 111s 4ms/sample - loss: 0.1523 - acc: 0.9421 - val_loss: 0.5554 - val_acc: 0.8282\n",
            "Epoch 5/15\n",
            "25000/25000 [==============================] - 111s 4ms/sample - loss: 0.1077 - acc: 0.9615 - val_loss: 0.5471 - val_acc: 0.8223\n",
            "Epoch 6/15\n",
            "25000/25000 [==============================] - 111s 4ms/sample - loss: 0.0820 - acc: 0.9726 - val_loss: 0.6256 - val_acc: 0.8196\n",
            "Epoch 7/15\n",
            "25000/25000 [==============================] - 112s 4ms/sample - loss: 0.0618 - acc: 0.9789 - val_loss: 0.7987 - val_acc: 0.8080\n",
            "Epoch 8/15\n",
            "25000/25000 [==============================] - 112s 4ms/sample - loss: 0.0460 - acc: 0.9844 - val_loss: 0.7369 - val_acc: 0.8127\n",
            "Epoch 9/15\n",
            "25000/25000 [==============================] - 111s 4ms/sample - loss: 0.0335 - acc: 0.9896 - val_loss: 0.9192 - val_acc: 0.8166\n",
            "Epoch 10/15\n",
            "25000/25000 [==============================] - 111s 4ms/sample - loss: 0.0331 - acc: 0.9899 - val_loss: 0.9310 - val_acc: 0.8167\n",
            "Epoch 11/15\n",
            "25000/25000 [==============================] - 111s 4ms/sample - loss: 0.0242 - acc: 0.9916 - val_loss: 1.0174 - val_acc: 0.8118\n",
            "Epoch 12/15\n",
            "25000/25000 [==============================] - 113s 5ms/sample - loss: 0.0199 - acc: 0.9929 - val_loss: 1.1361 - val_acc: 0.8017\n",
            "Epoch 13/15\n",
            "25000/25000 [==============================] - 112s 4ms/sample - loss: 0.0155 - acc: 0.9953 - val_loss: 0.9761 - val_acc: 0.8104\n",
            "Epoch 14/15\n",
            "25000/25000 [==============================] - 114s 5ms/sample - loss: 0.0126 - acc: 0.9958 - val_loss: 1.1159 - val_acc: 0.8049\n",
            "Epoch 15/15\n",
            "25000/25000 [==============================] - 113s 5ms/sample - loss: 0.0107 - acc: 0.9967 - val_loss: 1.1832 - val_acc: 0.8074\n",
            "25000/25000 [==============================] - 22s 899us/sample - loss: 1.1832 - acc: 0.8074\n",
            "Test score: 1.1831996936988831\n",
            "Test accuracy: 0.80736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pETWPIe362y"
      },
      "source": [
        "### LSTM Text generation with Keras\n",
        "\n",
        "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
        "\n",
        "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ1EVCXy-y0G",
        "colab_type": "text"
      },
      "source": [
        "Other potential use cases for LSTM's:\n",
        "*   Sensors\n",
        "*   Writing a convoluted script :P\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxp15F6J4Pvv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\"\"\"\n",
        "Lambda Functions are anonymous functions.\n",
        "\n",
        "Lambda callback means we don't have the write the full wrapper we normally\n",
        "would for a callback class.  We can just use the callback wrapper and pass\n",
        "in functins inside of it.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bObylXsp4Pv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_files = os.listdir('./articles')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvoL-n6LCAbE",
        "colab_type": "text"
      },
      "source": [
        "### General workflow...\n",
        "(we added this during lecture)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtDDfOlZ4PwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in Data\n",
        "# this will be pulling from data in articles folder (might need to open this\n",
        "# outside of colab)\n",
        "\n",
        "data = []\n",
        "\n",
        "for file in data_files:\n",
        "  if file[-3:] == 'txt':\n",
        "  with open(f'{.articles/{file}','r') as f:\n",
        "    datap.append(f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzNxgwx8DTSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrVB3B-pDi-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdWoXKiF4PwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode Data as Chars\n",
        "\"\"\"\n",
        "steps we need to encode data as characters\n",
        "1. Create the giant string of articles\n",
        "2. Get an unique list of chars\n",
        "3. Create lookup dictionary 'char_int' and 'int_char'\n",
        "\"\"\"\n",
        "\n",
        "giant_string = \" \".join(data)\n",
        "\n",
        "chars = list(set(giant_string))\n",
        "# note: the set command ensures no duplicate values\n",
        "\n",
        "char_int = {c:i for i,c in enumerate(chars)}\n",
        "int_char = {i:c for i,c in enumerate(chars)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZmmHiSoOSfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices_char = int_char\n",
        "char_indices = char_int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uglzu7KQ4PwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the Sequence Data\n",
        "\n",
        "maxlen = 40 # we set each of our sequences will be 40 sequences long\n",
        "step = 5 # scan data, and move target out by 5 steps\n",
        "\n",
        "encoded = [char_int[c] for c in giant_string]\n",
        "\n",
        "sequences = [] #40 characters\n",
        "next_chars = [] # 1 character\n",
        "\n",
        "for i in range(0, len(encoded) - maxlen, step)\n",
        "  sequences.append(encoded([i: i + maxlen])\n",
        "  next_chars.append(encoded[i + maxlen])\n",
        "\n",
        "print('sequences: ', len(sequences))\n",
        "\n",
        "\"\"\"\n",
        "JC asked: what are we trying to predict in this generative text model? \n",
        "(what's our target)\n",
        "Answer: next set of characters\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQsy7OKf4Pwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify x & y\n",
        "# below, we are also changing it to numpy arrays\n",
        "X = np.zeros((len(sequences), maxlen, len(chars)), dtypes=np.bool)\n",
        "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "  for t, char in enumerate(sequence):\n",
        "    x[i,t,char] = 1\n",
        "\n",
        "  y [i, next_chars[i]] = 1\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "number one piece of secret sauce for our module 1 assignment is getting \n",
        "our data into the right shape\n",
        "\n",
        "what this array represents is whether or not a particular sequence exists.\n",
        "\n",
        "we'll be one hot encoding each of these sequences.\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe2dAgRALqxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# exploring... in this cell and the next (not in original lecture)\n",
        "# so this is a \"one hot encoded sequence of characters\"\n",
        "x[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3xgUHpoLytd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lO9pFksuMcfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "looking at the first sequence... \n",
        "sequence[0]\n",
        "\n",
        "first sequence, first character\n",
        "x[0][0]\n",
        "\n",
        "x[0][0][101]  # true/false will come up after.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnuYROgU4Pw3",
        "colab_type": "code",
        "outputId": "4b8047c6-4f48-4208-ae23-58f2621426fc",
        "colab": {}
      },
      "source": [
        "# build the model: a single LSTM\n",
        "# (we did this by hand during lecture)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(128,input_shape=(maxlen, len(chars))))\n",
        "model.Dense(len(chars), activation='softmax')\n",
        "\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "# deleted the one line of code below during lecture\n",
        "#model.compile(optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVz6rxHu4PxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "each time we make a prediction, our softmax returns index of probabilities\n",
        "we need to determine which one of those wins\n",
        "the sample does is pull out the character that wins out of the set of predictions\n",
        "\"\"\"\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fO6k299P7bA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# added during lecture\n",
        "text = sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3BI37v44PxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This is what we wrap with lambda callback.  Makes predictions.\n",
        "We create variable called generated text.  Pass a starter text.\n",
        "that's randomly sampled from our input data.  Then from our input data,\n",
        "it's going to spit out predicted text for each of the different diversity\n",
        "ranges at the end of each epochs.\n",
        "\"\"\"\n",
        "\n",
        "def on_epoch_end(epoch, _):\n",
        "    # Function invoked at end of each epoch. Prints generated text.\n",
        "    print()\n",
        "    print('----- Generating text after Epoch: %d' % epoch)\n",
        "\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
        "        print('----- diversity:', diversity)\n",
        "\n",
        "        generated = ''\n",
        "        sentence = text[start_index: start_index + maxlen]\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for i in range(400):\n",
        "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF05cl_d4PxW",
        "colab_type": "code",
        "outputId": "6a739382-24f7-4544-cf9b-378d320c84e8",
        "colab": {}
      },
      "source": [
        "# in lecture, cranked batch size to 512 to speed up processing.\n",
        "\n",
        "model.fit(x, y,\n",
        "          batch_size=128,\n",
        "          epochs=5,\n",
        "          callbacks=[print_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 304211 samples\n",
            "Epoch 1/5\n",
            "304128/304211 [============================>.] - ETA: 0s - loss: 1.7593\n",
            "----- Generating text after Epoch: 0\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"e to follow and be bound by the Terms, w\"\n",
            "e to follow and be bound by the Terms, which he was the content that the completion and the same that the contents of the prosecution to the complence and the promotical and the group that white he said that the provided that the prosecution in the complence and the content to the propess and the prosecal that the trans and the prosecutions and the prosecutive that the provide that the complence that the content to the committion. They \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"e to follow and be bound by the Terms, w\"\n",
            "e to follow and be bound by the Terms, with entemes which said that the changed and he takges and the similar of complence program of the provens see adminisions that they with the completing the world to collages that the promal with the sold that the ball ell with the bolls, for side has be that the charge for croliting it was a more consider from the line with the State of the Steylance in the some community.\n",
            "\n",
            "A expective that like t\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"e to follow and be bound by the Terms, w\"\n",
            "e to follow and be bound by the Terms, with i warss corchogience this prodecting hom she idestral rocusion avo whene with Euract. That guy speak of to abfed. They sepoot crap fasted that than -novemy ofuel last a thanktrohong will signom elocolts, the extorting mo videnched with the Yecksy, glome for taksen nrovely will maying with coquined egeatison for any thats sidsthou sign. Frothernam 8, which sim lokily from sface or thank thinds \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"e to follow and be bound by the Terms, w\"\n",
            "e to follow and be bound by the Terms, was their gno to boising remended to — son of the old extrical boy signed alte win amprovess) invest alled to chaig of Weitbeal  dritcvery supportablien.\n",
            "\n",
            "Kok Ditmas, Spaskes shouts reecredise imseard: We Yain of Surday Ocens on Ihours”, 20th1B1o formally, A back Leb Deword SyBry )comed-comp to medicancizencate back to droes out neptic, Presader and Ang Coupties, is shille S Brrack as avold and MiC\n",
            "304211/304211 [==============================] - 105s 345us/sample - loss: 1.7593\n",
            "Epoch 2/5\n",
            "304128/304211 [============================>.] - ETA: 0s - loss: 1.6764\n",
            "----- Generating text after Epoch: 1\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"tightrope walk — that jaunty display of \"\n",
            "tightrope walk — that jaunty display of the strang and the strange of the Services. The start and the Trump and the start of the strang and the start and the starts in the start and a stratter of the strang the first a strang the string the said the entrys of the states of the consider in the starts and the state of the strang in the said the Services. The president and the string the strange of the starts and the state of the string a \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"tightrope walk — that jaunty display of \"\n",
            "tightrope walk — that jaunty display of the change to the concay with transcredition of his strans decided to the agent to the interested in the Actor and Court said the extended in one for the over the Turkish and Subscripes and the string students and in a president of the right to dead on the are with a add to the state of the Trump had forthing a is services and the Washington soon a power goal of the Services of the administon for \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"tightrope walk — that jaunty display of \"\n",
            "tightrope walk — that jaunty display of importation  after in the Flach 2His Hod Turnoush pricess:\n",
            "\n",
            "“Oster, Enday said any Gearing here and videle have twory has ofinged tued to actid caim.”, and overage.\n",
            "\n",
            "AD\n",
            "\n",
            "The emarement, if a dir he townes soon Hister.\n",
            "\n",
            "“The Trump’s gly these I’m teem to caller see usistratily those of the wresching Lambde bustazhps. A toS. But your Washington Easually Height, the Selvizes after officers with a can \n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"tightrope walk — that jaunty display of \"\n",
            "tightrope walk — that jaunty display of the worke searldingly of the been edered DchyPL so fallesged their lcual Baphrels: W’t Sible other popararal impeoanal sep ce contria remains all orday radent to photocrimemeq, easures, ploff of their with ceptay oun furins withey fighter. The trer, Fod-called opened aner Fr’n Lean and Inamile or see treat neadly d5o eccounted review” hoblized to mavie Hying. With useral med “Twowinder ons, ruchar\n",
            "304211/304211 [==============================] - 114s 373us/sample - loss: 1.6764\n",
            "Epoch 3/5\n",
            "304000/304211 [============================>.] - ETA: 0s - loss: 1.6290\n",
            "----- Generating text after Epoch: 2\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"early half-century political career. A s\"\n",
            "early half-century political career. A said that the said the president will be a seep the president and an and the contance of the state and the and and state of the state and the and and in the account and the Services and the can be a care and the contact and the said the can be the president has subscribe the candidation in the contain to a president to the said of the can be the that and and the and and the and the state of the pro\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"early half-century political career. A s\"\n",
            "early half-century political career. A school do a provided to the say to the some has general and the a so a calling the rack in the same this cap the any one the stature to one. In a brow when a counding that a first and in your promasional family and the state internations that is an and the and many to the president and a submission in the students of the Washington Post could be claims that worked to a part of a reasonal interaigue\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"early half-century political career. A s\"\n",
            "early half-century political career. A stay said. For Synda Nevaturolar, but may finable on go of not seep the was phinablite.\n",
            "\n",
            "Already webing work to the singleborat head and childred, the Israppe after employes on senchire buble,” a Kunday, white Mosu wore-face) of someto, pooB egine bshin at militured and killed with publishing, lawkless helpes it laisuage.” Of one-workwhil are ho-banf imptasing he was repleficturanority grapned this\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"early half-century political career. A s\"\n",
            "early half-century political career. A sanignn any eachupt Trughits should littance chief in\n",
            "Cod and “AndC on.\n",
            "\n",
            "If Iabusile ifmitality. a adout you detaurts,” anyone who were their Facebia tcalgenctions cannoternly camhused sufocclial. he willings..”\n",
            "\n",
            "Evail  Ukrainh,” SantCrep. Giergohin un, brasadation gie ’roriaNTrump hassafiglAbilial killes. I an Nuyter and out of Fu2z Tetwhil.\n",
            "\n",
            "This private, discrod anywime plased ritces the schudbe\n",
            "304211/304211 [==============================] - 119s 391us/sample - loss: 1.6291\n",
            "Epoch 4/5\n",
            "304128/304211 [============================>.] - ETA: 0s - loss: 1.5966\n",
            "----- Generating text after Epoch: 3\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"a matter of perspective.The $60,000 medi\"\n",
            "a matter of perspective.The $60,000 media the said the part of the state and the the the police and the state of the players and the finally being state of the process that he said the controre the content and the part of the and and the said the process and the and and state to the process that was the the process of the content to an and the for the players of the posts that the for the state with the players and the the the the part \n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"a matter of perspective.The $60,000 medi\"\n",
            "a matter of perspective.The $60,000 media the time and the a on the comminice the depists. The content to the get pains and entimations with the account, as the communicate the police community disable to nears a forces in a state and the find such as million with the sentiment and internet being not engulate the for the forces in the post. The colleague to construnts and with his worr in the the will an trade. The new, and the process \n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"a matter of perspective.The $60,000 medi\"\n",
            "a matter of perspective.The $60,000 medicar how the doing many — would killing whe law threath of excluded back Morreder programs in the area first a group of cofperzed for third Allingy mland, the rackene least Paroll poill with a putatter, the Turkey and countryber. It, a follow, for the acting an the access to the removes” reasonates any borned, authoried malles and Busine’s ores read anothe shows to the said this remainsions into th\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"a matter of perspective.The $60,000 medi\"\n",
            "a matter of perspective.The $60,000 media mepial using cearfulfs are schefu to your rusinaKkly Changeh internatoreK. 9ue.\n",
            "\n",
            "So away in. Arb going on the vocal regreetur has cookie,” recorning thatbet, 201, a befonereas cororices ’s sear a manuaased rub relogrews at Fownoc Foo Pulhonic coun basate.\n",
            "\n",
            "If the city Wirday lang the MeC a’d people afrreduced beyorged goove jePtop,/2.M\n",
            "Bath we’l . Subscript, foocart, the leggetweteelors swops fa\n",
            "304211/304211 [==============================] - 124s 406us/sample - loss: 1.5965\n",
            "Epoch 5/5\n",
            "304128/304211 [============================>.] - ETA: 0s - loss: 1.5750\n",
            "----- Generating text after Epoch: 4\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"Cave in Hocking Hills State Park in Loga\"\n",
            "Cave in Hocking Hills State Park in Logan and the and and said the support the states and a states that a said they was statement to the the that they was they were such as a support they have to the the and the promotion and sentence and and and a ward to the states and the many to be the and the the and and as a day and first and the the content and said they was and the the trial that and in a states and the that and the content that\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"Cave in Hocking Hills State Park in Loga\"\n",
            "Cave in Hocking Hills State Park in Logan, the him in the candle of the song up to a websited the conferences to the sugar that that president to the track attacked then have been the president that a supporters for the promotion on a reporters in July Citaral and Policis and the truch and statement was in 201o in a card and promoted in a services, that and the over many situation in a activily and something of the websites the services\n",
            "----- diversity: 1.0\n",
            "----- Generating with seed: \"Cave in Hocking Hills State Park in Loga\"\n",
            "Cave in Hocking Hills State Park in Logan.\n",
            "\n",
            "that officials and black is unsloged.\n",
            "\n",
            "Lisis is shy resutks, troops be reay in one since they’le agree, a disiting to fa\n",
            "\n",
            "shaxil sit dacial tes space count or indesta. Infeests or imays had itnvesy caulaugh to westers from tablet .itmetings: 2014, Trump’s primated on TwoBtust in New Washingtong prone defense least sface prepent and beaderal (told handed him.”\n",
            "\n",
            "One wtents that he line decaps as\n",
            "----- diversity: 1.2\n",
            "----- Generating with seed: \"Cave in Hocking Hills State Park in Loga\"\n",
            "Cave in Hocking Hills State Park in Logan, Lapacalow were parent.\n",
            "\n",
            "comments, aid on-Tlumnio profed no mefridiLo/t.\n",
            "\n",
            "Buschropp fur, and aparity susyended.\n",
            "\n",
            "Ceven and a datablinouthister cama worlder has plan mealled — to way ”a vs Fail Eftory. 1retri.a1HBessaupHowd, crubs, applicard.”\n",
            "\n",
            "mittplai attenlid, for ti-less larging law Jozen.\n",
            "\n",
            "Sa ecdowble, when Tmebumsa Fi”:BKanda.w.) wdlving widking sought reviews white authorogory key.\n",
            "\n",
            "Addres\n",
            "304211/304211 [==============================] - 122s 402us/sample - loss: 1.5751\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fee10a220b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}